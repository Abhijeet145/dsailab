{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a087b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (768, 9)\n",
      "Training data shape: (614, 8)\n",
      "Testing data shape: (154, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "column_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\", \"Outcome\"]\n",
    "df = pd.read_csv('./diabetes.csv', names=column_names)\n",
    "df.drop(index=df.index[0], axis=0, inplace=True)\n",
    "\n",
    "X = df.drop(\"Outcome\", axis=1)\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29781f65",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "GaussianNB model is used. No need of Laplacian correction here as the model assumes all data to follow normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e03b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Accuracy: 0.7662337662337663\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81        99\n",
      "           1       0.66      0.71      0.68        55\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.75      0.75      0.75       154\n",
      "weighted avg       0.77      0.77      0.77       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Training\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cea3e5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae46c48",
   "metadata": {},
   "source": [
    "# Linear Regression with Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eafb2303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Linear Regression Accuracy: 0.6688311688311688\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.77        99\n",
      "           1       0.57      0.29      0.39        55\n",
      "\n",
      "    accuracy                           0.67       154\n",
      "   macro avg       0.63      0.58      0.58       154\n",
      "weighted avg       0.65      0.67      0.63       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Training\n",
    "sgd_model = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
    "sgd_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "\n",
    "# evaluation\n",
    "print(\"SGD Linear Regression Accuracy:\", accuracy_score(y_test, y_pred_sgd))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf48a1",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772c0d0",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c488e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7467532467532467\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80        99\n",
      "           1       0.64      0.67      0.65        55\n",
      "\n",
      "    accuracy                           0.75       154\n",
      "   macro avg       0.73      0.73      0.73       154\n",
      "weighted avg       0.75      0.75      0.75       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Training\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ecb86f",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c14b43",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour\n",
    "\n",
    "Using:\n",
    " - Euclidean Distance\n",
    " - Manhattan Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3afa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN (Euclidean) Accuracy: 0.6623376623376623\n",
      "KNN (Manhattan) Accuracy: 0.6688311688311688\n",
      "Classification Report (Euclidean):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.71      0.73        99\n",
      "           1       0.52      0.58      0.55        55\n",
      "\n",
      "    accuracy                           0.66       154\n",
      "   macro avg       0.64      0.64      0.64       154\n",
      "weighted avg       0.67      0.66      0.67       154\n",
      "\n",
      "Classification Report (Manhattan):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74        99\n",
      "           1       0.53      0.58      0.56        55\n",
      "\n",
      "    accuracy                           0.67       154\n",
      "   macro avg       0.64      0.65      0.65       154\n",
      "weighted avg       0.68      0.67      0.67       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# KNN with Euclidean distance\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\")\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_knn_euclidean = knn_euclidean.predict(X_test)\n",
    "\n",
    "# KNN with Manhattan distance\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric=\"manhattan\")\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_knn_manhattan = knn_manhattan.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"KNN (Euclidean) Accuracy:\", accuracy_score(y_test, y_pred_knn_euclidean))\n",
    "print(\"KNN (Manhattan) Accuracy:\", accuracy_score(y_test, y_pred_knn_manhattan))\n",
    "print(\"Classification Report (Euclidean):\\n\", classification_report(y_test, y_pred_knn_euclidean))\n",
    "print(\"Classification Report (Manhattan):\\n\", classification_report(y_test, y_pred_knn_manhattan))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4faead3",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590d090",
   "metadata": {},
   "source": [
    "# Decision Tree using Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1dd2df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Entropy) Accuracy: 0.7207792207792207\n",
      "Classification Report (Entropy):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78        99\n",
      "           1       0.60      0.64      0.62        55\n",
      "\n",
      "    accuracy                           0.72       154\n",
      "   macro avg       0.70      0.70      0.70       154\n",
      "weighted avg       0.72      0.72      0.72       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Decision Tree with Information Gain\n",
    "dt_entropy = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "dt_entropy.fit(X_train, y_train)\n",
    "y_pred_dt_entropy = dt_entropy.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Decision Tree (Entropy) Accuracy:\", accuracy_score(y_test, y_pred_dt_entropy))\n",
    "print(\"Classification Report (Entropy):\\n\", classification_report(y_test, y_pred_dt_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cabc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
